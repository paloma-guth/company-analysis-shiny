---
title: "data_collection"
---

## Econ data

```{r}
#| include: FALSE

library(tidyverse)
library(stringr)
library(rvest)
library(httr)
library(jsonlite)
library(tidytext)
library(stringr)

afinn<-get_sentiments("afinn")

robotstxt::paths_allowed("https://stockanalysis.com/stocks/nflx/financials/balance-sheet/")
#cheked - WSJ allow data scrapping
```

#### Data collection

```{r}
library(rvest)

scrape_data <- function(url) {
  webpage <- read_html(url)
  tables <- html_nodes(webpage, css = "table") 
  dataset <- html_table(tables, header = TRUE, fill = TRUE)[[1]]
  return(dataset)
}

terms <- c("nflx", "dis", "wbd", "tsla", "tm", "hmc", "googl", "meta", "bidu", "adm", "agro", "fdp", "cvs", "ci", "elv")
types <- c("balance-sheet", "cash-flow-statement")

scrape_all_data <- function(terms, types) {
  for (term in terms) {
    for (type in types) {
      standard_url <- str_c("https://stockanalysis.com/stocks/",term,"/financials/",type,"/")
      dataset <- scrape_data(standard_url)
      assign(paste0(term, "_", type), dataset, envir = .GlobalEnv)
      #save to csv - in case
      #dataset_name <- paste0(term, "_", type, ".csv")  # Generate unique filename
      #write.csv(dataset, file = dataset_name, row.names = FALSE)
    }
  }
}

scrape_all_data(terms, types)

#all tables would need to be pivot wider - all messed up now

```


## Sentimental analysis data - NYT api

```{r}
library(nytimes)

nytimes_key("csHR8NIDuVovzJ9A15iyiEmR7aVAGBK9")

archive <- ny_archive(2023, 2)

main_headlines <- list()

for(i in archive){
  main_headline <- i$headline$main
  main_headlines <- c(main_headlines, main_headline)
}

main_headlines_df <- data.frame(Main_Headline = unlist(main_headlines))
str_detect(main_headlines_df$Main_Headline, "Officers")
main_headlines_df[str_detect(main_headlines_df$Main_Headline, "Tesla"), ]


```
```{r}
library(stringr)

filter_headlines <- function(terms, years, months) {
  
  filtered_headlines_list <- list()
  
  for (term in terms) {

    filtered_headlines_year_list <- list()
    

    for (year in years) {

      for (month in months) {

        archive <- ny_archive(year, month)
        
        main_headlines <- lapply(archive, function(article) article$headline$main)
        filtered_headlines <- main_headlines[str_detect(main_headlines, term)]
        
        if (length(filtered_headlines) > 0) {
          filtered_headlines_year_list <- c(filtered_headlines_year_list, filtered_headlines)
        }
      }
    }
    
    # Convert the list of filtered headlines for each year into a data frame
    if (length(filtered_headlines_year_list) > 0) {
      filtered_headlines_df <- data.frame(Main_Headline = unlist(filtered_headlines_year_list))
      
      assign(paste0("filtered_headlines_", term), filtered_headlines_df, envir = .GlobalEnv)
      
      # Save the filtered headlines data frame for the current term
      filtered_headlines_list[[term]] <- filtered_headlines_df
    }
  }
  
  # Return the list of filtered headlines for each term
  return(filtered_headlines_list)
}

# Example usage:
terms <- c("Tesla")  # Add different terms to filter by
years <- 2021  # Add multiple years
months <- 1:4  # Add multiple months

# Call the function to filter headlines for each term and save them into separate data frames
filtered_headlines <- filter_headlines(terms, years, months)

# Access filtered headlines for a specific term
filtered_headlines[["Tesla"]]
 

```


## Geolocation

```{r}
#get location of headquarters for each company
#leaflet graph - gps location
```
