---
title: "data_collection"
---

## Econ data

```{r}
#| include: FALSE

library(tidyverse)
library(stringr)
library(rvest)
library(httr)
library(jsonlite)
library(tidytext)
library(stringr)

afinn<-get_sentiments("afinn")

robotstxt::paths_allowed("https://stockanalysis.com/stocks/nflx/financials/balance-sheet/")
#cheked - WSJ allow data scrapping
```

#### Data collection

```{r}
library(rvest)

scrape_data <- function(url) {
  webpage <- read_html(url)
  tables <- html_nodes(webpage, css = "table") 
  dataset <- html_table(tables, header = TRUE, fill = TRUE)[[1]]
  return(dataset)
}

terms <- c("nflx", "dis", "wbd", "tsla", "tm", "hmc", "googl", "meta", "bidu", "adm", "agro", "fdp", "cvs", "ci", "elv")
types <- c("balance-sheet", "cash-flow-statement")

scrape_all_data <- function(terms, types) {
  for (term in terms) {
    for (type in types) {
      standard_url <- str_c("https://stockanalysis.com/stocks/",term,"/financials/",type,"/")
      dataset <- scrape_data(standard_url)
      assign(paste0(term, "_", type), dataset, envir = .GlobalEnv)
      #save to csv - in case
      #dataset_name <- paste0(term, "_", type, ".csv")  # Generate unique filename
      #write.csv(dataset, file = dataset_name, row.names = FALSE)
    }
  }
}

scrape_all_data(terms, types)

#all tables would need to be pivot wider - all messed up now

```


## Sentimental analysis data - NYT api

```{r}
library(nytimes)

nytimes_key("csHR8NIDuVovzJ9A15iyiEmR7aVAGBK9")

archive<-ny_archive(year, month)
year<-2023
month<-1:4

archive <- ny_archive(2023, 2)

main_headlines <- list()

for(i in archive){
  main_headline <- i$headline$main
  main_headlines <- c(main_headlines, main_headline)
}

tesla_headlines<-main_headlines_df[str_detect(main_headlines_df$Main_Headline, "Tesla"), ]
tesla_headlines_df <- data.frame(tesla_headlines)

```



## Geolocation

```{r}
#get location of headquarters for each company
#leaflet graph - gps location
```
