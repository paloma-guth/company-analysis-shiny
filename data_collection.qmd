---
title: "data_collection"
---

## Econ data

```{r}
#| include: FALSE

library(tidyverse)
library(stringr)
library(rvest)
library(httr)
library(jsonlite)
library(tidytext)
library(stringr)
library(dplyr)
library(readr)
library(janitor)

#afinn<-get_sentiments("afinn")

robotstxt::paths_allowed("https://stockanalysis.com/stocks/nflx/financials/balance-sheet/")
#checked - WSJ allow data scrapping
```

#### Data collection and cleaning

```{r}
cleaning_data <- function(data) {
  data_long <- data |>
    pivot_longer(cols = -Year, names_to = "Variable", values_to = "Value")
  
  data_wide <- data_long |>
    pivot_wider(names_from = Year, values_from = Value)
  
  data_wide[data_wide == "-"] <- NA 
  
  data_wide <- data_wide |>
    select(where(~ !any(is.na(.))))|>
    filter(!rowSums(across(everything(), ~. == "Upgrade")))|>
    clean_names(case='snake')|>
    rename_with(~paste0(.x, "_percent"), where(~any(str_detect(., "%"))))|> 
    mutate(across(everything(), parse_number))|>
    rename(year=variable)
  
  return(data_wide)
}

scrape_data <- function(url) {
  webpage <- read_html(url)
  tables <- html_nodes(webpage, css = "table") 
  dataset <- html_table(tables, header = TRUE, fill = TRUE)[[1]]
  return(dataset)
}

terms <- c("nflx", "dis", "wbd", "tsla", "tm", "hmc", "googl", "meta", "bidu", "adm", "agro", "fdp", "cvs", "ci", "elv")
types <- c("balance-sheet", "cash-flow-statement")
tidy_type<-c("balance_sheet", "cash_flow") #couldn't find another way to have the dataframe name in a tidy format

scrape_all_data <- function(terms, types, tidy_types) {
  
  for (term in terms) {
    for (type in types) {
      standard_url <- str_c("https://stockanalysis.com/stocks/",term,"/financials/",type,"/")
      dataset <- scrape_data(standard_url)
      clean_data<-cleaning_data(dataset)
      assign(paste0(term, "_", ifelse(type==types[1], tidy_types[1], tidy_types[2])), clean_data, envir = .GlobalEnv)
      #save to csv - in case
      #dataset_name <- paste0(term, "_", type, ".csv") 
      #write.csv(dataset, file = dataset_name, row.names = FALSE)
    }
  }
}

scrape_all_data(terms, types, tidy_type)

```


#### New data bases

```{r}
test<-bind_cols(wbd_cash_flow$net_cash_flow, tsla_cash_flow$net_cash_flow, tm_cash_flow$net_cash_flow, nflx_cash_flow$net_cash_flow, meta_cash_flow$net_cash_flow, hmc_cash_flow$net_cash_flow, googl_cash_flow$net_cash_flow, fdp_cash_flow$net_cash_flow, elv_cash_flow$net_cash_flow, dis_cash_flow$net_cash_flow, cvs_cash_flow$net_cash_flow, ci_cash_flow$net_cash_flow, bidu_cash_flow$net_cash_flow, agro_cash_flow$net_cash_flow, adm_cash_flow$net_cash_flow)

original_names <- rep("net_cash_flow", 15)

# Add dataset names as prefixes to the variable names
colnames(test) <- paste0(c("wbd", "tsla", "tm", "nflx", "meta", "hmc", "googl", "fdp", "elv", "dis", "cvs", "ci", "bidu", "agro", "adm"), "_", original_names)

```

```{r}
#working on making this a function so we can have a dataframe for each variable we want a graph about

#fill up the NAs? - mean from the other years we have?

datasets <- list(
  wbd_cash_flow, tsla_cash_flow, tm_cash_flow, nflx_cash_flow, meta_cash_flow,
  hmc_cash_flow, googl_cash_flow, fdp_cash_flow, elv_cash_flow, dis_cash_flow,
  cvs_cash_flow, ci_cash_flow, bidu_cash_flow, agro_cash_flow, adm_cash_flow
)

net_cash_flows <- lapply(datasets, function(x) select(x, net_cash_flow, year))

year_ranges <- lapply(net_cash_flows, function(x) range(x$year))
overall_range <- range(do.call(c, year_ranges))
all_years <- seq(overall_range[1], overall_range[2])


net_cash_flows_filled <- lapply(net_cash_flows, function(df) {
  df %>%
    complete(year = all_years) %>%
    arrange(year)
})

merged_case_flow <- net_cash_flows_filled[[1]]

for(i in 2:length(net_cash_flows_filled)) {
  merged_case_flow <- bind_cols(merged_case_flow, net_cash_flows_filled[[i]][2])
}

original_names <- rep("net_cash_flow", length(datasets))

colnames(merged_case_flow)[-1] <- paste0(c("wbd", "tsla", "tm", "nflx", "meta", "hmc", "googl", "fdp", "elv", "dis", "cvs", "ci", "bidu", "agro", "adm"), "_", original_names)

write.csv(merged_case_flow, file = "merged_case_flow.csv", row.names = FALSE)


#test for a graph with the data - not looking good
#normalize in any form?
graph_data <- merged_case_flow %>%
  pivot_longer(cols = -year, names_to = "company", values_to = "net_cash_flow")

# Plot
ggplot(graph_data, aes(x = year, y = net_cash_flow, color = company)) +
  geom_line() +
  labs(x = "Year", y = "Net Cash Flow", color = "Company") +
  theme_minimal()

```




## Sentimental analysis data - NYT api

```{r}
library(nytimes)

nytimes_key("csHR8NIDuVovzJ9A15iyiEmR7aVAGBK9")

archive <- ny_archive(2023, 2)

main_headlines <- list()

for(i in archive){
  main_headline <- i$headline$main
  main_headlines <- c(main_headlines, main_headline)
}

main_headlines_df <- data.frame(Main_Headline = unlist(main_headlines))
str_detect(main_headlines_df$Main_Headline, "Officers")
main_headlines_df[str_detect(main_headlines_df$Main_Headline, "Tesla"), ]
```


## Geolocation

```{r}
#get location of headquarters for each company
#leaflet graph - gps location
```
