---
title: "data_collection"
---

## Econ data

```{r}
#| include: FALSE

library(tidyverse)
library(stringr)
library(rvest)
library(httr)
library(jsonlite)
library(tidytext)
library(stringr)
library(dplyr)
library(readr)
library(janitor)

#afinn<-get_sentiments("afinn")

robotstxt::paths_allowed("https://stockanalysis.com/stocks/nflx/financials/balance-sheet/")
#checked - WSJ allow data scrapping
```

#### Data collection and cleaning

```{r}
cleaning_data <- function(data) {
  data_long <- data |>
    pivot_longer(cols = -Year, names_to = "Variable", values_to = "Value")
  
  data_wide <- data_long |>
    pivot_wider(names_from = Year, values_from = Value)
  
  data_wide[data_wide == "-"] <- NA
  
  data_wide <- data_wide |>
    select(where(~ !any(is.na(.))))|>#would the janitor package do it in a more efficient way?
    filter(!rowSums(across(everything(), ~. == "Upgrade")))|>
    clean_names(case='snake')
  
  colnames(data_wide) <- ifelse(str_detect(colnames(data_wide), "%"), paste0(colnames(data_wide), "_percent"), colnames(data_wide))
  
  #data_wide<-data_wide|>
  #  mutate_if(~ any(str_detect(., "%")), ~str_replace_all(., "%", ""))
  
  #parse number - everything is still as char
  
  #percentage? - change variable name or *100
  
  
  return(data_wide)
}

scrape_data <- function(url) {
  webpage <- read_html(url)
  tables <- html_nodes(webpage, css = "table") 
  dataset <- html_table(tables, header = TRUE, fill = TRUE)[[1]]
  return(dataset)
}

terms <- c("nflx", "dis", "wbd", "tsla", "tm", "hmc", "googl", "meta", "bidu", "adm", "agro", "fdp", "cvs", "ci", "elv")
types <- c("balance-sheet", "cash-flow-statement")
tidy_type<-c("balance_sheet", "cash_flow") #couldn't find another way to have the dataframe name in a tidy format

scrape_all_data <- function(terms, types, tidy_types) {
  
  for (term in terms) {
    for (type in types) {
      standard_url <- str_c("https://stockanalysis.com/stocks/",term,"/financials/",type,"/")
      dataset <- scrape_data(standard_url)
      clean_data<-cleaning_data(dataset)
      assign(paste0(term, "_", ifelse(type==types[1], tidy_types[1], tidy_types[2])), clean_data, envir = .GlobalEnv)
      #save to csv - in case
      #dataset_name <- paste0(term, "_", type, ".csv")  # Generate unique filename
      #write.csv(dataset, file = dataset_name, row.names = FALSE)
    }
  }
}

scrape_all_data(terms, types, tidy_type)

```


## Sentimental analysis data - NYT api

```{r}
library(nytimes)

nytimes_key("csHR8NIDuVovzJ9A15iyiEmR7aVAGBK9")

archive <- ny_archive(2023, 2)

main_headlines <- list()

for(i in archive){
  main_headline <- i$headline$main
  main_headlines <- c(main_headlines, main_headline)
}

main_headlines_df <- data.frame(Main_Headline = unlist(main_headlines))
str_detect(main_headlines_df$Main_Headline, "Officers")
main_headlines_df[str_detect(main_headlines_df$Main_Headline, "Tesla"), ]
```


```{r}
cash_fl_google <-`googl_cash-flow-statement`|>
  pivot_longer(cols= c("Net Income","Depreciation", "Share_Compensation", "Other_Activities"),
               names_to = "Category", 
               values_to = "Amount")

cash_fl_google
```


## Geolocation

```{r}
#get location of headquarters for each company
#leaflet graph - gps location
```
